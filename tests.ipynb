{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hotgrad.variable import Variable\n",
    "\n",
    "import torch\n",
    "from torch import FloatTensor\n",
    "from torch.autograd import Variable as tVariable\n",
    "from torch.nn.functional import relu, tanh\n",
    "\n",
    "from numpy import isclose\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from dataset_generator import generate_dataset, generate_dataset_aux\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modules to be tested:\n",
    "\n",
    "**Operands**:\n",
    "- <font color=green>add</font>\n",
    "- <font color=green>sub</font>\n",
    "- <font color=green>mul</font>\n",
    "- <font color=green>matmul</font>\n",
    "- <font color=green>pow</font>\n",
    "- <font color=green>mean</font>\n",
    "\n",
    "**Layers**:\n",
    "- <font color=green>Linear</font>\n",
    "\n",
    "**Losses**:\n",
    "- <font color=green>MSE</font>\n",
    "\n",
    "**Activations**:\n",
    "- <font color=green>ReLU</font>\n",
    "- <font color=green>Tanh</font>\n",
    "\n",
    "**Optimizers**:\n",
    "- SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoding=True\n",
    "X_train, X_test, y_train, y_test = generate_dataset(one_hot_encoding=one_hot_encoding)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "if one_hot_encoding:\n",
    "    plt.scatter(X_train.data[:, 0], X_train.data[:, 1], c=y_train.data[:, 0])\n",
    "    plt.show()\n",
    "    plt.scatter(X_test.data[:, 0], X_test.data[:, 1], c=y_test.data[:, 0])\n",
    "else:\n",
    "    plt.scatter(X_train.data[:, 0], X_train.data[:, 1], c=y_train.data)\n",
    "    plt.show()\n",
    "    plt.scatter(X_test.data[:, 0], X_test.data[:, 1], c=y_test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test logistic regression, NearestNeighbors and random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = generate_dataset(one_hot_encoding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-Logistic regression:\")\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train.data, y_train.data)\n",
    "print(\"\\tTrain score:\", model.score(X_train.data, y_train.data))\n",
    "print(\"\\tTest score:\", model.score(X_test.data, y_test.data))\n",
    "\n",
    "print(\"\\n-KMeans:\")\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(X_train.data, y_train.data)\n",
    "print(\"\\tTrain score:\", model.score(X_train.data, y_train.data))\n",
    "print(\"\\tTest score:\", model.score(X_test.data, y_test.data))\n",
    "\n",
    "print(\"\\n-Random Forest:\")\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train.data, y_train.data)\n",
    "print(\"\\tTrain score:\", model.score(X_train.data, y_train.data))\n",
    "print(\"\\tTest score:\", model.score(X_test.data, y_test.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain(var1, var2, var3, var4, var5):\n",
    "    temp = (var1.pow(2) + var2 - var3 - var3).pow(3)\n",
    "    \n",
    "    if isinstance(var1, Variable):\n",
    "        temp = temp.relu()\n",
    "    else:\n",
    "        temp = relu(temp)\n",
    "    temp = ((temp*var1) @ var4) - var5\n",
    "\n",
    "    if isinstance(var1, Variable):\n",
    "        temp = temp.tanh()\n",
    "    else:\n",
    "        temp = tanh(temp)\n",
    "\n",
    "    temp = ((temp + var5) * var5).pow(3)\n",
    "    return temp.mean()\n",
    "\n",
    "max_ = 10\n",
    "n_tests = 10\n",
    "\n",
    "correct = True\n",
    "for i in range(n_tests):\n",
    "    dim1 = np.random.randint(1, max_)\n",
    "    dim2 = np.random.randint(1, max_)\n",
    "    dim3 = np.random.randint(1, max_)\n",
    "\n",
    "    var1 = FloatTensor(dim1, dim2).uniform_(-100, 100) \n",
    "    var2 = FloatTensor(dim1, dim2).uniform_(-100, 100) \n",
    "    var3 = FloatTensor(dim1, dim2).uniform_(-100, 100) \n",
    "    var4 = FloatTensor(dim2, dim3).uniform_(-100, 100) \n",
    "    var5 = FloatTensor(dim1, dim3).uniform_(-100, 100) \n",
    "\n",
    "    a, b, c, d, e = Variable(var1, requires_grad=True), Variable(var2, requires_grad=True), Variable(var3, requires_grad=True), Variable(var4, requires_grad=True), Variable(var5, requires_grad=True)\n",
    "    res = chain(a, b, c, d, e)\n",
    "\n",
    "    ta, tb, tc, td, te = tVariable(var1, requires_grad=True), tVariable(var2, requires_grad=True), tVariable(var3, requires_grad=True), tVariable(var4, requires_grad=True), tVariable(var5, requires_grad=True)\n",
    "    tres = chain(ta, tb, tc, td, te)\n",
    "\n",
    "    res.backward()\n",
    "    tres.backward()\n",
    "\n",
    "    correct = correct and isclose(res.data, tres.data, atol=0).all()\n",
    "    correct = correct and isclose(a.grad, ta.grad.data, atol=0).all()\n",
    "    correct = correct and isclose(b.grad, tb.grad.data, atol=0).all()\n",
    "    correct = correct and isclose(c.grad, tc.grad.data, atol=0).all()\n",
    "    correct = correct and isclose(d.grad, td.grad.data, atol=0).all()\n",
    "    \n",
    "    if not correct:\n",
    "        print(\"ERROR! :(\")\n",
    "        break\n",
    "        \n",
    "if correct:\n",
    "    print(\"good boy! :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Linear and MSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hotgrad.functions.layers import Linear\n",
    "from hotgrad.functions.losses import MSE\n",
    "from hotgrad.optimizers import SGD\n",
    "\n",
    "from torch.nn import Linear as tLinear\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim.sgd import SGD as tSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ = 100\n",
    "dim1 = np.random.randint(1, max_)\n",
    "dim2 = np.random.randint(1, max_)\n",
    "dim3 = np.random.randint(1, max_)\n",
    "dim4 = np.random.randint(1, max_)\n",
    "\n",
    "var1 = FloatTensor(dim1, dim2).uniform_(-100, 100) \n",
    "\n",
    "target1 = FloatTensor(dim1, dim4).uniform_(-100, 100)\n",
    "\n",
    "a = Variable(var1, requires_grad=True)\n",
    "o = Variable(target1)\n",
    "l1 = Linear(var1.shape[1], dim3)\n",
    "l2 = Linear(dim3, dim4)\n",
    "mse = MSE()\n",
    "\n",
    "ta = tVariable(var1, requires_grad=True)\n",
    "to = tVariable(target1)\n",
    "tl1 = tLinear(var1.shape[1], dim3, bias=False)\n",
    "tl2 = tLinear(dim3, dim4, bias=False)\n",
    "tmse = MSELoss()\n",
    "\n",
    "l1.weight.data = tl1.weight.data.t()\n",
    "l2.weight.data = tl2.weight.data.t()\n",
    "\n",
    "res = mse(l2(l1(a)), o)\n",
    "tres = tmse(tl2(tl1(ta)), to)\n",
    "\n",
    "res.backward()\n",
    "tres.backward()\n",
    "\n",
    "sgd = SGD(lr=0.1)\n",
    "sgd.set_params(list(l1.params()) + list(l2.params()))\n",
    "tsgd = tSGD(list(tl1.parameters()) + list(tl2.parameters()), lr=0.1)\n",
    "\n",
    "before = l1.weight.data.clone()\n",
    "tbefore = tl1.weight.clone()\n",
    "\n",
    "sgd.step()\n",
    "tsgd.step()\n",
    "\n",
    "print((a.grad == ta.grad.data).all())\n",
    "# print(isclose(l1.weight.grad, tl1.weight.grad.data.t()).all())\n",
    "# print(isclose(l2.weight.grad, tl2.weight.grad.data.t()).all())\n",
    "print(isclose(l1.weight.data, tl1.weight.data.t()).all())\n",
    "print(isclose(l2.weight.data, tl2.weight.data.t()).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tl1.weight.data-tbefore.data)/tl1.weight.grad.data, (l1.weight.data-before)/l1.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hotgrad.sequential import Sequential\n",
    "from hotgrad.functions.layers import Linear\n",
    "from hotgrad.functions.activations import ReLU, Tanh\n",
    "from hotgrad.functions.losses import MSE\n",
    "from hotgrad.optimizers import SGD\n",
    "\n",
    "from dataset_generator import generate_dataset\n",
    "\n",
    "# generate the dataset\n",
    "X_train, X_test, y_train, y_test = generate_dataset(1000, one_hot_encoding=True)\n",
    "\n",
    "# model: two input units, two output units, three hidden layers of 25 units\n",
    "# model = Sequential([Linear(2,25), ReLU(), Linear(25,25), ReLU(), Linear(25,25), ReLU(), Linear(25,2)], MSE(), SGD(lr=0.01))\n",
    "model = Sequential([Linear(2,25), Tanh(), Linear(25,25), Tanh(), Linear(25,2), ReLU()], MSE(), SGD(lr=0.01))\n",
    "                   \n",
    "# fitting the model\n",
    "model.fit(X_train, y_train, X_test, y_test, epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
