{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "from torch import FloatTensor, is_tensor\n",
    "from hotgrad.functions import layers, activations, losses, operands\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.autograd import Variable as tVariable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generalLoss = MSELoss()\n",
    "#generalLoss\n",
    "crossEntropy = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TheBestItalianDeepFramework.losses.MSE at 0x7fc2eaeaccf8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_loss = losses.MSE()\n",
    "mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.1327\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomTensor1, randomTensor2 = Tensor(3,1).normal_(), Tensor(3,1).normal_()\n",
    "randomTensor1, randomTensor2\n",
    "mse_loss.forward(randomTensor1, randomTensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(randomTensor1 - randomTensor2).pow(2).mean(0), mse_loss.forward(randomTensor1, randomTensor2), generalLoss(Variable(randomTensor1), Variable(randomTensor2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Variable((3, 4)), Tensor((3, 4))\n",
    "tensor = Tensor(20,5)\n",
    "linearModule = linear.Linear(5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 8.5616e+12  4.5636e-41 -4.5888e+05  3.0798e-41\n",
       " 3.3129e-18  7.2646e+22  7.2250e+28  2.5226e-18\n",
       " 2.3877e-18  2.6302e+20  6.1949e-04  2.5640e-09\n",
       " 1.6408e-07  3.0957e+12  6.9363e-07  2.5790e-09\n",
       " 2.6659e-09  1.2610e+16  5.4916e-34  1.5411e-31\n",
       "[variable.Variable of size 5x4]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linearModule.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = randomTensor1.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t += 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "1.00000e-02 *\n",
       "  4.9787\n",
       "  4.9787\n",
       "  4.9787\n",
       "[torch.FloatTensor of size 3x1]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-(Tensor(t.size()).fill_(1) * Tensor(t.size()).fill_(3))).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1.0000e+00\n",
       " 1.0000e+00\n",
       " 1.3456e+09\n",
       "[torch.FloatTensor of size 3x1]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import exp\n",
    "t.exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test basic operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [python operators](https://docs.python.org/2/library/operator.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hotgrad.variable import Variable\n",
    "from torch import FloatTensor\n",
    "from torch.autograd import Variable as tVariable\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test multiplication\n",
    "x = FloatTensor(1).random_(100)\n",
    "y = FloatTensor(1).random_(100)\n",
    "\n",
    "a = Variable(x, requires_grad=True)\n",
    "b = Variable(y, requires_grad=True)\n",
    "c = a*b\n",
    "c.backward()\n",
    "\n",
    "ta = tVariable(x, requires_grad=True)\n",
    "tb = tVariable(y, requires_grad=True)\n",
    "tc = ta*tb\n",
    "tc.backward()\n",
    "\n",
    "print(ta.grad, tb.grad)\n",
    "print(a.grad, b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1  1  1  1\n",
      " 1  1  1  1\n",
      "[torch.ByteTensor of size 2x4]\n",
      "\n",
      "\n",
      " 1  1  1  1\n",
      " 1  1  1  1\n",
      "[torch.ByteTensor of size 2x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = FloatTensor(2, 4).random_(100)\n",
    "y = FloatTensor(2, 4).random_(100)\n",
    "# print(x, y)\n",
    "\n",
    "a = Variable(x, requires_grad=True)\n",
    "b = Variable(y, requires_grad=True)\n",
    "c = (a*b).mean()\n",
    "c.backward()\n",
    "\n",
    "\n",
    "ta = tVariable(x, requires_grad=True)\n",
    "tb = tVariable(y, requires_grad=True)\n",
    "tc = (ta*tb).mean()\n",
    "tc.backward()\n",
    "\n",
    "# print(ta.grad, tb.grad)\n",
    "# print(a.grad, b.grad)\n",
    "print(ta.grad.data == a.grad)\n",
    "print(tb.grad.data == b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mul\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  20\n",
       "  40\n",
       " [torch.FloatTensor of size 2], Mul operator:\n",
       " \n",
       " ------ Left operand ------\n",
       " Variable containing:\n",
       "  2\n",
       "  4\n",
       " [torch.FloatTensor of size 2]\n",
       " --------------------------\n",
       " \n",
       " ------ Right operand ------\n",
       " Variable containing:\n",
       "  10\n",
       " [torch.FloatTensor of size 1]\n",
       " --------------------------)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Variable(FloatTensor([2, 4]))\n",
    "b = Variable(FloatTensor([10]))\n",
    "c = a*b\n",
    "\n",
    "c, c.previous_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-264-4cbf2af4d645>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mgrad_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mgrad_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_variables\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, user_create_graph)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 new_grads.append(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "a = tVariable(FloatTensor([2, 5]), requires_grad=True)\n",
    "b = tVariable(FloatTensor([10]), requires_grad=True)\n",
    "c = a*b\n",
    "c.mean().backward()\n",
    "a.grad, b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub\n",
      "add\n",
      "div\n",
      "matmul\n",
      "pow\n"
     ]
    }
   ],
   "source": [
    "a = Variable(FloatTensor([2]))\n",
    "b = Variable(FloatTensor([10]))\n",
    "a*b\n",
    "a-b\n",
    "a+b\n",
    "a/b\n",
    "a@b\n",
    "a**4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "from torch import FloatTensor, is_tensor\n",
    "from hotgrad.functions import layers, activations, losses, operands\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.autograd import Variable as tVariable\n",
    "\n",
    "from hotgrad.variable import Variable\n",
    "from torch import FloatTensor\n",
    "from torch.autograd import Variable as tVariable\n",
    "from hotgrad.functions.losses import MSE\n",
    "from hotgrad.sequential import Sequential\n",
    "from hotgrad.optimizers import SGD\n",
    "\n",
    "my_mse = MSE()\n",
    "mse_comp = MSELoss()\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "\n",
    "a = Variable(FloatTensor([1, 2, 6, 4]), requires_grad=True)\n",
    "b = Variable(FloatTensor([2,3,4,5]), requires_grad=True)\n",
    "# b = FloatTensor([2,3,4,5])\n",
    "# b = 2\n",
    "# c = a.pow(b)\n",
    "# c = a ** b\n",
    "#d = a.mse(b)\n",
    "#d = my_mse.forward(a, b)\n",
    "d = my_mse(a, b)\n",
    "#d = c.mean()\n",
    "\n",
    "a2 = tVariable(FloatTensor([1, 2, 6, 4]), requires_grad=True)\n",
    "b2 = tVariable(FloatTensor([2, 3, 4, 5]))\n",
    "c2 = a2.pow(b2)\n",
    "d2 = mse_comp(a2, b2)\n",
    "#d22 = c2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  1.7500\n",
       " [torch.FloatTensor of size 1], Variable containing:\n",
       "  1.7500\n",
       " [torch.FloatTensor of size 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.backward()\n",
    "d2.backward()\n",
    "d, d2, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.6500\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input_grad = FloatTensor([1])\n",
    "d.backward()\n",
    "d2.backward()\n",
    "#d.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  1\n",
       " [torch.FloatTensor of size 1], Variable containing:\n",
       "  1\n",
       " [torch.FloatTensor of size 1], \n",
       " -0.5000\n",
       " -0.5000\n",
       " -0.5000\n",
       " -0.5000\n",
       " [torch.FloatTensor of size 4], Variable containing:\n",
       " -0.5000\n",
       " -0.5000\n",
       " -0.5000\n",
       " -0.5000\n",
       " [torch.FloatTensor of size 4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#d, d22, a.grad, a2.grad\n",
    "d, d, a.grad, a2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "asd = FloatTensor([1, 2, 6, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       "[torch.FloatTensor of size 4]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asd.fill_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MSE operator:\n",
       " \n",
       " ------ Left operand ------\n",
       " None\n",
       " --------------------------\n",
       " \n",
       " ------ Right operand ------\n",
       " None\n",
       " --------------------------, <hotgrad.optimizers.SGD at 0x7fe9e9c65cc0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = MSE()\n",
    "optimizer = SGD()\n",
    "\n",
    "#seq = Sequential(, loss, optimizer)\n",
    "\n",
    "loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss: 763.0776977539062. Train accuracy 0.49%. \n",
      "Epoch 1: Train loss: 3168.2333984375. Train accuracy 0.51%. \n",
      "Epoch 2: Train loss: 1706.2052001953125. Train accuracy 0.49%. \n",
      "Epoch 3: Train loss: 152.90280151367188. Train accuracy 0.49%. \n",
      "Epoch 4: Train loss: 36.53561019897461. Train accuracy 0.49%. \n",
      "Epoch 5: Train loss: 21.107162475585938. Train accuracy 0.49%. \n",
      "Epoch 6: Train loss: 14.106332778930664. Train accuracy 0.49%. \n",
      "Epoch 7: Train loss: 10.18343734741211. Train accuracy 0.49%. \n",
      "Epoch 8: Train loss: 7.738353729248047. Train accuracy 0.49%. \n",
      "Epoch 9: Train loss: 6.117164134979248. Train accuracy 0.49%. \n",
      "Epoch 10: Train loss: 4.986887454986572. Train accuracy 0.49%. \n",
      "Epoch 11: Train loss: 4.210071086883545. Train accuracy 0.49%. \n",
      "Epoch 12: Train loss: 3.6306309700012207. Train accuracy 0.49%. \n",
      "Epoch 13: Train loss: 3.188429832458496. Train accuracy 0.49%. \n",
      "Epoch 14: Train loss: 2.8430442810058594. Train accuracy 0.49%. \n",
      "Epoch 15: Train loss: 2.565656900405884. Train accuracy 0.50%. \n",
      "Epoch 16: Train loss: 2.3401355743408203. Train accuracy 0.50%. \n",
      "Epoch 17: Train loss: 2.1538643836975098. Train accuracy 0.50%. \n",
      "Epoch 18: Train loss: 1.9964245557785034. Train accuracy 0.50%. \n",
      "Epoch 19: Train loss: 1.8646152019500732. Train accuracy 0.49%. \n",
      "Epoch 20: Train loss: 1.7509843111038208. Train accuracy 0.50%. \n",
      "Epoch 21: Train loss: 1.6525846719741821. Train accuracy 0.50%. \n",
      "Epoch 22: Train loss: 1.5653389692306519. Train accuracy 0.50%. \n",
      "Epoch 23: Train loss: 1.486838936805725. Train accuracy 0.49%. \n",
      "Epoch 24: Train loss: 1.4158945083618164. Train accuracy 0.49%. \n",
      "Epoch 25: Train loss: 1.3523592948913574. Train accuracy 0.49%. \n",
      "Epoch 26: Train loss: 1.2943519353866577. Train accuracy 0.49%. \n",
      "Epoch 27: Train loss: 1.2411019802093506. Train accuracy 0.49%. \n",
      "Epoch 28: Train loss: 1.1912429332733154. Train accuracy 0.49%. \n",
      "Epoch 29: Train loss: 1.1451311111450195. Train accuracy 0.50%. \n",
      "Epoch 30: Train loss: 1.1026129722595215. Train accuracy 0.50%. \n",
      "Epoch 31: Train loss: 1.0635473728179932. Train accuracy 0.50%. \n",
      "Epoch 32: Train loss: 1.0272290706634521. Train accuracy 0.49%. \n",
      "Epoch 33: Train loss: 0.9932225942611694. Train accuracy 0.49%. \n",
      "Epoch 34: Train loss: 0.9616191387176514. Train accuracy 0.50%. \n",
      "Epoch 35: Train loss: 0.9320256114006042. Train accuracy 0.50%. \n",
      "Epoch 36: Train loss: 0.903756320476532. Train accuracy 0.50%. \n",
      "Epoch 37: Train loss: 0.8769798278808594. Train accuracy 0.50%. \n",
      "Epoch 38: Train loss: 0.8516371846199036. Train accuracy 0.50%. \n",
      "Epoch 39: Train loss: 0.8275477290153503. Train accuracy 0.50%. \n",
      "Epoch 40: Train loss: 0.8047521710395813. Train accuracy 0.50%. \n",
      "Epoch 41: Train loss: 0.7832915782928467. Train accuracy 0.50%. \n",
      "Epoch 42: Train loss: 0.7633315920829773. Train accuracy 0.50%. \n",
      "Epoch 43: Train loss: 0.7445912957191467. Train accuracy 0.50%. \n",
      "Epoch 44: Train loss: 0.7269532680511475. Train accuracy 0.50%. \n",
      "Epoch 45: Train loss: 0.7103076577186584. Train accuracy 0.50%. \n",
      "Epoch 46: Train loss: 0.6946227550506592. Train accuracy 0.50%. \n",
      "Epoch 47: Train loss: 0.6798400282859802. Train accuracy 0.50%. \n",
      "Epoch 48: Train loss: 0.6659255027770996. Train accuracy 0.50%. \n",
      "Epoch 49: Train loss: 0.652909517288208. Train accuracy 0.50%. \n"
     ]
    }
   ],
   "source": [
    "from hotgrad.variable import Variable\n",
    "from hotgrad.sequential import Sequential\n",
    "from hotgrad.functions.layers import Linear\n",
    "from hotgrad.functions.activations import ReLU, Tanh\n",
    "from hotgrad.functions.losses import MSE\n",
    "from hotgrad.optimizers import SGD\n",
    "\n",
    "from dataset_generator import generate_dataset\n",
    "\n",
    "# generate the dataset\n",
    "X_train, X_test, y_train, y_test = generate_dataset(1000, one_hot_encoding=True)\n",
    "\n",
    "# model: two input units, two output units, three hidden layers of 25 units\n",
    "modules = [Linear(2,25), ReLU(), Linear(25,25), ReLU(), Linear(25,25), ReLU(), Linear(25,2)]\n",
    "# model = Sequential([Linear(2,25), ReLU(), Linear(25,25), ReLU(), Linear(25,25), ReLU(), Linear(25,2)], MSE(), SGD(lr=0.01))\n",
    "model = Sequential([Linear(2,25), ReLU(), Linear(25,25), ReLU(), Linear(25,25), ReLU(), Linear(25,2)], MSE(), SGD(lr=0.002))\n",
    "\n",
    "# fitting the model\n",
    "model.fit(X_train, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward of MatMul\n",
      "backward of MatMul\n",
      "backward of MatMul\n",
      "backward of MatMul\n"
     ]
    }
   ],
   "source": [
    "output = model.forward(X_train)\n",
    "loss = model.loss_criterion(output, y_train)\n",
    "model.zero_grad()\n",
    "loss.backward()\n",
    "before = list(map(lambda x: x.data.clone(), model.params()))\n",
    "model.optimizer.step()\n",
    "after = model.params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_classes = y_train.data.max(1)[1] if y_train.data.dim() == 2 else y_train.data\n",
    "predicted = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "  0\n",
       "  1\n",
       "  1\n",
       "  0\n",
       "  1\n",
       " [torch.LongTensor of size 5], Variable containing:\n",
       "  1  0\n",
       "  0  1\n",
       "  0  1\n",
       "  1  0\n",
       "  0  1\n",
       " [torch.FloatTensor of size 5x2], \n",
       "  1\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       " [torch.LongTensor of size 5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_classes[:5], y_train[:5], predicted[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
