{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "from torch import FloatTensor, is_tensor\n",
    "from hotgrad.functions import layers, activations, losses, operands\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.autograd import Variable as tVariable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generalLoss = MSELoss()\n",
    "#generalLoss\n",
    "crossEntropy = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TheBestItalianDeepFramework.losses.MSE at 0x7fc2eaeaccf8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_loss = losses.MSE()\n",
    "mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.1327\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomTensor1, randomTensor2 = Tensor(3,1).normal_(), Tensor(3,1).normal_()\n",
    "randomTensor1, randomTensor2\n",
    "mse_loss.forward(randomTensor1, randomTensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(randomTensor1 - randomTensor2).pow(2).mean(0), mse_loss.forward(randomTensor1, randomTensor2), generalLoss(Variable(randomTensor1), Variable(randomTensor2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Variable((3, 4)), Tensor((3, 4))\n",
    "tensor = Tensor(20,5)\n",
    "linearModule = linear.Linear(5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 8.5616e+12  4.5636e-41 -4.5888e+05  3.0798e-41\n",
       " 3.3129e-18  7.2646e+22  7.2250e+28  2.5226e-18\n",
       " 2.3877e-18  2.6302e+20  6.1949e-04  2.5640e-09\n",
       " 1.6408e-07  3.0957e+12  6.9363e-07  2.5790e-09\n",
       " 2.6659e-09  1.2610e+16  5.4916e-34  1.5411e-31\n",
       "[variable.Variable of size 5x4]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linearModule.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = randomTensor1.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t += 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "1.00000e-02 *\n",
       "  4.9787\n",
       "  4.9787\n",
       "  4.9787\n",
       "[torch.FloatTensor of size 3x1]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-(Tensor(t.size()).fill_(1) * Tensor(t.size()).fill_(3))).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1.0000e+00\n",
       " 1.0000e+00\n",
       " 1.3456e+09\n",
       "[torch.FloatTensor of size 3x1]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import exp\n",
    "t.exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test basic operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [python operators](https://docs.python.org/2/library/operator.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hotgrad.variable import Variable\n",
    "from torch import FloatTensor\n",
    "from torch.autograd import Variable as tVariable\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test multiplication\n",
    "x = FloatTensor(1).random_(100)\n",
    "y = FloatTensor(1).random_(100)\n",
    "\n",
    "a = Variable(x, requires_grad=True)\n",
    "b = Variable(y, requires_grad=True)\n",
    "c = a*b\n",
    "c.backward()\n",
    "\n",
    "ta = tVariable(x, requires_grad=True)\n",
    "tb = tVariable(y, requires_grad=True)\n",
    "tc = ta*tb\n",
    "tc.backward()\n",
    "\n",
    "print(ta.grad, tb.grad)\n",
    "print(a.grad, b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1  1  1  1\n",
      " 1  1  1  1\n",
      "[torch.ByteTensor of size 2x4]\n",
      "\n",
      "\n",
      " 1  1  1  1\n",
      " 1  1  1  1\n",
      "[torch.ByteTensor of size 2x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = FloatTensor(2, 4).random_(100)\n",
    "y = FloatTensor(2, 4).random_(100)\n",
    "# print(x, y)\n",
    "\n",
    "a = Variable(x, requires_grad=True)\n",
    "b = Variable(y, requires_grad=True)\n",
    "c = (a*b).mean()\n",
    "c.backward()\n",
    "\n",
    "\n",
    "ta = tVariable(x, requires_grad=True)\n",
    "tb = tVariable(y, requires_grad=True)\n",
    "tc = (ta*tb).mean()\n",
    "tc.backward()\n",
    "\n",
    "# print(ta.grad, tb.grad)\n",
    "# print(a.grad, b.grad)\n",
    "print(ta.grad.data == a.grad)\n",
    "print(tb.grad.data == b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mul\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  20\n",
       "  40\n",
       " [torch.FloatTensor of size 2], Mul operator:\n",
       " \n",
       " ------ Left operand ------\n",
       " Variable containing:\n",
       "  2\n",
       "  4\n",
       " [torch.FloatTensor of size 2]\n",
       " --------------------------\n",
       " \n",
       " ------ Right operand ------\n",
       " Variable containing:\n",
       "  10\n",
       " [torch.FloatTensor of size 1]\n",
       " --------------------------)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Variable(FloatTensor([2, 4]))\n",
    "b = Variable(FloatTensor([10]))\n",
    "c = a*b\n",
    "\n",
    "c, c.previous_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-264-4cbf2af4d645>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mgrad_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mgrad_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_variables\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, user_create_graph)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 new_grads.append(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "a = tVariable(FloatTensor([2, 5]), requires_grad=True)\n",
    "b = tVariable(FloatTensor([10]), requires_grad=True)\n",
    "c = a*b\n",
    "c.mean().backward()\n",
    "a.grad, b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub\n",
      "add\n",
      "div\n",
      "matmul\n",
      "pow\n"
     ]
    }
   ],
   "source": [
    "a = Variable(FloatTensor([2]))\n",
    "b = Variable(FloatTensor([10]))\n",
    "a*b\n",
    "a-b\n",
    "a+b\n",
    "a/b\n",
    "a@b\n",
    "a**4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "from torch import FloatTensor, is_tensor\n",
    "from hotgrad.functions import layers, activations, losses, operands\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.autograd import Variable as tVariable\n",
    "\n",
    "from hotgrad.variable import Variable\n",
    "from torch import FloatTensor\n",
    "from torch.autograd import Variable as tVariable\n",
    "from hotgrad.functions.losses import MSE\n",
    "from hotgrad.sequential import Sequential\n",
    "from hotgrad.optimizers import SGD\n",
    "\n",
    "my_mse = MSE()\n",
    "mse_comp = MSELoss()\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "\n",
    "a = Variable(FloatTensor([1, 2, 6, 4]), requires_grad=True)\n",
    "b = Variable(FloatTensor([2,3,4,5]), requires_grad=True)\n",
    "# b = FloatTensor([2,3,4,5])\n",
    "# b = 2\n",
    "# c = a.pow(b)\n",
    "# c = a ** b\n",
    "#d = a.mse(b)\n",
    "#d = my_mse.forward(a, b)\n",
    "d = my_mse(a, b)\n",
    "#d = c.mean()\n",
    "\n",
    "a2 = tVariable(FloatTensor([1, 2, 6, 4]), requires_grad=True)\n",
    "b2 = tVariable(FloatTensor([2, 3, 4, 5]))\n",
    "c2 = a2.pow(b2)\n",
    "d2 = mse_comp(a2, b2)\n",
    "#d22 = c2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  1.7500\n",
       " [torch.FloatTensor of size 1], \n",
       "  1.6500\n",
       " [torch.FloatTensor of size 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d, d2.data.sub_(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.6500\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input_grad = FloatTensor([1])\n",
    "d.backward()\n",
    "d2.backward()\n",
    "#d.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  1\n",
       " [torch.FloatTensor of size 1], Variable containing:\n",
       "  1\n",
       " [torch.FloatTensor of size 1], \n",
       " -0.5000\n",
       " -0.5000\n",
       " -0.5000\n",
       " -0.5000\n",
       " [torch.FloatTensor of size 4], Variable containing:\n",
       " -0.5000\n",
       " -0.5000\n",
       " -0.5000\n",
       " -0.5000\n",
       " [torch.FloatTensor of size 4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#d, d22, a.grad, a2.grad\n",
    "d, d, a.grad, a2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "asd = FloatTensor([1, 2, 6, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       "[torch.FloatTensor of size 4]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asd.fill_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MSE operator:\n",
       " \n",
       " ------ Left operand ------\n",
       " None\n",
       " --------------------------\n",
       " \n",
       " ------ Right operand ------\n",
       " None\n",
       " --------------------------, <hotgrad.optimizers.SGD at 0x7fe9e9c65cc0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = MSE()\n",
    "optimizer = SGD()\n",
    "\n",
    "#seq = Sequential(, loss, optimizer)\n",
    "\n",
    "loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (layers.py, line 20)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/marco/anaconda3/envs/ipykernel_py3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2910\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-23420eb20e1b>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from hotgrad.functions.layers import Linear\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/marco/Documents/programming/deepFramework/hotgrad/functions/layers.py\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    def params(self):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from hotgrad.variable import Variable\n",
    "from hotgrad.sequential import Sequential\n",
    "from hotgrad.functions.layers import Linear\n",
    "from hotgrad.functions.activations import ReLU, Tanh\n",
    "from hotgrad.functions.losses import MSE\n",
    "from hotgrad.optimizers import SGD\n",
    "\n",
    "from dataset_generator import generate_dataset\n",
    "\n",
    "# generate the dataset\n",
    "X_train, X_test, y_train, y_test = generate_dataset(1000)\n",
    "\n",
    "# model: two input units, two output units, three hidden layers of 25 units\n",
    "modules = [Linear(2,25), ReLU(), Linear(25,25), ReLU(), Linear(25,25), ReLU(), Linear(25,2)]\n",
    "# model = Sequential([Linear(2,25), ReLU(), Linear(25,25), ReLU(), Linear(25,25), ReLU(), Linear(25,2)], MSE(), SGD(lr=0.01))\n",
    "model = Sequential([Linear(2,25), ReLU(), Linear(25,25), ReLU(), Linear(25,25), ReLU(), Linear(25,2)], MSE(), SGD(lr=0.01))\n",
    "\n",
    "# fitting the model\n",
    "model.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
