{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "from torch import FloatTensor, is_tensor\n",
    "from hotgrad.functions import layers, activations, losses, operands\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.autograd import Variable as tVariable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generalLoss = MSELoss()\n",
    "#generalLoss\n",
    "crossEntropy = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TheBestItalianDeepFramework.losses.MSE at 0x7fc2eaeaccf8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_loss = losses.MSE()\n",
    "mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.1327\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomTensor1, randomTensor2 = Tensor(3,1).normal_(), Tensor(3,1).normal_()\n",
    "randomTensor1, randomTensor2\n",
    "mse_loss.forward(randomTensor1, randomTensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(randomTensor1 - randomTensor2).pow(2).mean(0), mse_loss.forward(randomTensor1, randomTensor2), generalLoss(Variable(randomTensor1), Variable(randomTensor2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Variable((3, 4)), Tensor((3, 4))\n",
    "tensor = Tensor(20,5)\n",
    "linearModule = linear.Linear(5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 8.5616e+12  4.5636e-41 -4.5888e+05  3.0798e-41\n",
       " 3.3129e-18  7.2646e+22  7.2250e+28  2.5226e-18\n",
       " 2.3877e-18  2.6302e+20  6.1949e-04  2.5640e-09\n",
       " 1.6408e-07  3.0957e+12  6.9363e-07  2.5790e-09\n",
       " 2.6659e-09  1.2610e+16  5.4916e-34  1.5411e-31\n",
       "[variable.Variable of size 5x4]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linearModule.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = randomTensor1.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t += 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "1.00000e-02 *\n",
       "  4.9787\n",
       "  4.9787\n",
       "  4.9787\n",
       "[torch.FloatTensor of size 3x1]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-(Tensor(t.size()).fill_(1) * Tensor(t.size()).fill_(3))).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1.0000e+00\n",
       " 1.0000e+00\n",
       " 1.3456e+09\n",
       "[torch.FloatTensor of size 3x1]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import exp\n",
    "t.exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test basic operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [python operators](https://docs.python.org/2/library/operator.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hotgrad.variable import Variable\n",
    "from torch import FloatTensor\n",
    "from torch.autograd import Variable as tVariable\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test multiplication\n",
    "x = FloatTensor(1).random_(100)\n",
    "y = FloatTensor(1).random_(100)\n",
    "\n",
    "a = Variable(x, requires_grad=True)\n",
    "b = Variable(y, requires_grad=True)\n",
    "c = a*b\n",
    "c.backward()\n",
    "\n",
    "ta = tVariable(x, requires_grad=True)\n",
    "tb = tVariable(y, requires_grad=True)\n",
    "tc = ta*tb\n",
    "tc.backward()\n",
    "\n",
    "print(ta.grad, tb.grad)\n",
    "print(a.grad, b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1  1  1  1\n",
      " 1  1  1  1\n",
      "[torch.ByteTensor of size 2x4]\n",
      "\n",
      "\n",
      " 1  1  1  1\n",
      " 1  1  1  1\n",
      "[torch.ByteTensor of size 2x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = FloatTensor(2, 4).random_(100)\n",
    "y = FloatTensor(2, 4).random_(100)\n",
    "# print(x, y)\n",
    "\n",
    "a = Variable(x, requires_grad=True)\n",
    "b = Variable(y, requires_grad=True)\n",
    "c = (a*b).mean()\n",
    "c.backward()\n",
    "\n",
    "\n",
    "ta = tVariable(x, requires_grad=True)\n",
    "tb = tVariable(y, requires_grad=True)\n",
    "tc = (ta*tb).mean()\n",
    "tc.backward()\n",
    "\n",
    "# print(ta.grad, tb.grad)\n",
    "# print(a.grad, b.grad)\n",
    "print(ta.grad.data == a.grad)\n",
    "print(tb.grad.data == b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mul\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  20\n",
       "  40\n",
       " [torch.FloatTensor of size 2], Mul operator:\n",
       " \n",
       " ------ Left operand ------\n",
       " Variable containing:\n",
       "  2\n",
       "  4\n",
       " [torch.FloatTensor of size 2]\n",
       " --------------------------\n",
       " \n",
       " ------ Right operand ------\n",
       " Variable containing:\n",
       "  10\n",
       " [torch.FloatTensor of size 1]\n",
       " --------------------------)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Variable(FloatTensor([2, 4]))\n",
    "b = Variable(FloatTensor([10]))\n",
    "c = a*b\n",
    "\n",
    "c, c.previous_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-264-4cbf2af4d645>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mgrad_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mgrad_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_variables\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, user_create_graph)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 new_grads.append(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "a = tVariable(FloatTensor([2, 5]), requires_grad=True)\n",
    "b = tVariable(FloatTensor([10]), requires_grad=True)\n",
    "c = a*b\n",
    "c.mean().backward()\n",
    "a.grad, b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub\n",
      "add\n",
      "div\n",
      "matmul\n",
      "pow\n"
     ]
    }
   ],
   "source": [
    "a = Variable(FloatTensor([2]))\n",
    "b = Variable(FloatTensor([10]))\n",
    "a*b\n",
    "a-b\n",
    "a+b\n",
    "a/b\n",
    "a@b\n",
    "a**4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "from torch import FloatTensor, is_tensor\n",
    "from hotgrad.functions import layers, activations, losses, operands\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.autograd import Variable as tVariable\n",
    "\n",
    "from hotgrad.variable import Variable\n",
    "from torch import FloatTensor\n",
    "from torch.autograd import Variable as tVariable\n",
    "from hotgrad.functions.losses import MSE\n",
    "from hotgrad.sequential import Sequential\n",
    "from hotgrad.optimizers import SGD\n",
    "\n",
    "my_mse = MSE()\n",
    "mse_comp = MSELoss()\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "\n",
    "a = Variable(FloatTensor([1, 2, 6, 4]), requires_grad=True)\n",
    "b = Variable(FloatTensor([2,3,4,5]), requires_grad=True)\n",
    "# b = FloatTensor([2,3,4,5])\n",
    "# b = 2\n",
    "# c = a.pow(b)\n",
    "# c = a ** b\n",
    "#d = a.mse(b)\n",
    "#d = my_mse.forward(a, b)\n",
    "d = my_mse(a, b)\n",
    "#d = c.mean()\n",
    "\n",
    "a2 = tVariable(FloatTensor([1, 2, 6, 4]), requires_grad=True)\n",
    "b2 = tVariable(FloatTensor([2, 3, 4, 5]))\n",
    "c2 = a2.pow(b2)\n",
    "d2 = mse_comp(a2, b2)\n",
    "#d22 = c2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  1.7500\n",
       " [torch.FloatTensor of size 1], Variable containing:\n",
       "  1.7500\n",
       " [torch.FloatTensor of size 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.backward()\n",
    "d2.backward()\n",
    "d, d2, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.6500\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input_grad = FloatTensor([1])\n",
    "d.backward()\n",
    "d2.backward()\n",
    "#d.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  1\n",
       " [torch.FloatTensor of size 1], Variable containing:\n",
       "  1\n",
       " [torch.FloatTensor of size 1], \n",
       " -0.5000\n",
       " -0.5000\n",
       " -0.5000\n",
       " -0.5000\n",
       " [torch.FloatTensor of size 4], Variable containing:\n",
       " -0.5000\n",
       " -0.5000\n",
       " -0.5000\n",
       " -0.5000\n",
       " [torch.FloatTensor of size 4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#d, d22, a.grad, a2.grad\n",
    "d, d, a.grad, a2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "asd = FloatTensor([1, 2, 6, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       "[torch.FloatTensor of size 4]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asd.fill_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MSE operator:\n",
       " \n",
       " ------ Left operand ------\n",
       " None\n",
       " --------------------------\n",
       " \n",
       " ------ Right operand ------\n",
       " None\n",
       " --------------------------, <hotgrad.optimizers.SGD at 0x7fe9e9c65cc0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = MSE()\n",
    "optimizer = SGD()\n",
    "\n",
    "#seq = Sequential(, loss, optimizer)\n",
    "\n",
    "loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- fold 1/4 -----------------\n",
      "Epoch 0: Train loss: 1.7221800088882446. Train accuracy 0.50%. \n",
      "Epoch 1: Train loss: 1.7101560831069946. Train accuracy 0.50%. \n",
      "Epoch 2: Train loss: 1.6974973678588867. Train accuracy 0.50%. \n",
      "Epoch 3: Train loss: 1.684207558631897. Train accuracy 0.50%. \n",
      "Epoch 4: Train loss: 1.670312523841858. Train accuracy 0.50%. \n",
      "Epoch 5: Train loss: 1.6558458805084229. Train accuracy 0.50%. \n",
      "Epoch 6: Train loss: 1.640859603881836. Train accuracy 0.50%. \n",
      "Epoch 7: Train loss: 1.6254531145095825. Train accuracy 0.50%. \n",
      "Epoch 8: Train loss: 1.609761357307434. Train accuracy 0.50%. \n",
      "Epoch 9: Train loss: 1.5939304828643799. Train accuracy 0.50%. \n",
      "----------------- fold 1/4 -----------------\n",
      "Epoch 0: Train loss: 1.25308358669281. Train accuracy 0.34%. \n",
      "Epoch 1: Train loss: 1.2194130420684814. Train accuracy 0.35%. \n",
      "Epoch 2: Train loss: 1.1880013942718506. Train accuracy 0.37%. \n",
      "Epoch 3: Train loss: 1.1592906713485718. Train accuracy 0.39%. \n",
      "Epoch 4: Train loss: 1.1335643529891968. Train accuracy 0.40%. \n",
      "Epoch 5: Train loss: 1.110949158668518. Train accuracy 0.40%. \n",
      "Epoch 6: Train loss: 1.0914291143417358. Train accuracy 0.41%. \n",
      "Epoch 7: Train loss: 1.0748616456985474. Train accuracy 0.41%. \n",
      "Epoch 8: Train loss: 1.0610029697418213. Train accuracy 0.41%. \n",
      "Epoch 9: Train loss: 1.0495631694793701. Train accuracy 0.40%. \n",
      "----------------- fold 1/4 -----------------\n",
      "Epoch 0: Train loss: 1.2452964782714844. Train accuracy 0.51%. \n",
      "Epoch 1: Train loss: 1.2099331617355347. Train accuracy 0.51%. \n",
      "Epoch 2: Train loss: 1.1768242120742798. Train accuracy 0.51%. \n",
      "Epoch 3: Train loss: 1.1465210914611816. Train accuracy 0.51%. \n",
      "Epoch 4: Train loss: 1.1194146871566772. Train accuracy 0.51%. \n",
      "Epoch 5: Train loss: 1.0957196950912476. Train accuracy 0.51%. \n",
      "Epoch 6: Train loss: 1.0754910707473755. Train accuracy 0.51%. \n",
      "Epoch 7: Train loss: 1.0586315393447876. Train accuracy 0.51%. \n",
      "Epoch 8: Train loss: 1.0448485612869263. Train accuracy 0.51%. \n",
      "Epoch 9: Train loss: 1.0337551832199097. Train accuracy 0.51%. \n",
      "----------------- fold 1/4 -----------------\n",
      "Epoch 0: Train loss: 1.3381843566894531. Train accuracy 0.48%. \n",
      "Epoch 1: Train loss: 1.3119256496429443. Train accuracy 0.48%. \n",
      "Epoch 2: Train loss: 1.2823314666748047. Train accuracy 0.48%. \n",
      "Epoch 3: Train loss: 1.2487319707870483. Train accuracy 0.48%. \n",
      "Epoch 4: Train loss: 1.2111071348190308. Train accuracy 0.48%. \n",
      "Epoch 5: Train loss: 1.17030668258667. Train accuracy 0.48%. \n",
      "Epoch 6: Train loss: 1.1283938884735107. Train accuracy 0.48%. \n",
      "Epoch 7: Train loss: 1.0885467529296875. Train accuracy 0.48%. \n",
      "Epoch 8: Train loss: 1.054154634475708. Train accuracy 0.48%. \n",
      "Epoch 9: Train loss: 1.0274516344070435. Train accuracy 0.48%. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': [0.476, 0.386, 0.508, 0.518],\n",
       " 'train_score': [0.498, 0.404, 0.5073333333333333, 0.484]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hotgrad.variable import Variable\n",
    "from torch import FloatTensor\n",
    "from hotgrad.sequential import Sequential\n",
    "from hotgrad.functions.layers import Linear\n",
    "from hotgrad.functions.activations import ReLU, Tanh\n",
    "from hotgrad.functions.losses import MSE\n",
    "from hotgrad.optimizers import SGD\n",
    "import torch\n",
    "\n",
    "from dataset_generator import generate_dataset\n",
    "\n",
    "# generate the dataset\n",
    "X_train, X_test, y_train, y_test = generate_dataset(1000, one_hot_encoding=True)\n",
    "y_train.data = y_train.data*2-1\n",
    "y_test.data = y_test.data*2-1\n",
    "\n",
    "X_train.data = (X_train.data - X_train.data.mean())/X_train.data.std() \n",
    "X_test.data = (X_test.data - X_test.data.mean())/X_test.data.std()\n",
    "\n",
    "X_all = Variable(torch.cat((X_train.data, X_test.data), 0))\n",
    "y_all = Variable(torch.cat((y_train.data, y_test.data), 0))\n",
    "\n",
    "# model: two input units, two output units, three hidden layers of 25 units\n",
    "modules = [Linear(2,25), ReLU(), Linear(25,25), ReLU(), Linear(25,25), ReLU(), Linear(25,2)]\n",
    "# model = Sequential([Linear(2,25), ReLU(), Linear(25,25), ReLU(), Linear(25,25), ReLU(), Linear(25,2)], MSE(), SGD(lr=0.01))\n",
    "\n",
    "torch.manual_seed(4)\n",
    "# model = Sequential([Linear(2,25, 4), ReLU(), Linear(25,25, 4), ReLU(), Linear(25,25, 4), ReLU(), Linear(25,2, 4), Tanh()], MSE(), SGD(lr=0.01))\n",
    "model = Sequential([Linear(2,25), ReLU(), Linear(25,25), ReLU(), Linear(25,25), ReLU(), Linear(25,2), Tanh()], MSE(), SGD(lr=0.01))\n",
    "\n",
    "# fitting the model\n",
    "# model.fit(X_train, y_train, X_test, y_test, epochs=10)\n",
    "model.cross_validate(X_all, y_all, epochs=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "   -1     1\n",
       "   -1     1\n",
       "   -1     1\n",
       "     ⋮      \n",
       "    1    -1\n",
       "    1    -1\n",
       "    1    -1\n",
       "[torch.FloatTensor of size 1000x2]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.639"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.predict(X_test).type(FloatTensor) == (y_test.data[:, 1]==1).type(FloatTensor)).type(FloatTensor).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.0000 -0.9895\n",
       " 1.0000 -0.9999\n",
       " 1.0000 -1.0000\n",
       "       ⋮        \n",
       " 1.0000 -0.9923\n",
       " 1.0000 -1.0000\n",
       " 1.0000  1.0000\n",
       "[torch.FloatTensor of size 1000x2]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward of MatMul\n",
      "backward of MatMul\n",
      "backward of MatMul\n",
      "backward of MatMul\n"
     ]
    }
   ],
   "source": [
    "output = model.forward(X_train)\n",
    "loss = model.loss_criterion(output, y_train)\n",
    "model.zero_grad()\n",
    "loss.backward()\n",
    "before = list(map(lambda x: x.data.clone(), model.params()))\n",
    "model.optimizer.step()\n",
    "after = model.params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_classes = y_train.data.max(1)[1] if y_train.data.dim() == 2 else y_train.data\n",
    "predicted = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "# import dlc_bci as bci\n",
    "from types import SimpleNamespace \n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# baselines\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import torch \n",
    "from torch.nn import Conv2d, Linear, functional, Module, CrossEntropyLoss\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def plot_scores(param, paramName, tr_scores, va_scores, log_scale=False):\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.grid()\n",
    "    plt.plot(param, tr_scores)\n",
    "    plt.plot(param, va_scores)\n",
    "    plt.legend(['train', 'validation'])\n",
    "    plt.xlabel(paramName)\n",
    "    plt.ylabel('Accuracy')\n",
    "    if log_scale:\n",
    "        plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lambda: 1e-06\n",
      "Test score: 0.49\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9d955c9b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "lambdas = np.logspace(-6, 6, 20) # grid search on a parameter of the model\n",
    "\n",
    "# here we store all the scores obtained with the different lambdas\n",
    "logreg = {\n",
    "    \"tr_scores\": [],\n",
    "    \"va_scores\": []\n",
    "}\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    result = cross_validate(LogisticRegression(C=lambda_), X_train.data.numpy(), (y_train.data == 1)[:, 1].numpy(), cv=10, return_train_score=True)\n",
    "    \n",
    "    logreg[\"tr_scores\"].append(np.mean(result[\"train_score\"]))\n",
    "    logreg[\"va_scores\"].append(np.mean(result[\"test_score\"]))\n",
    "    \n",
    "plot_scores(lambdas, \"lambda\", logreg[\"tr_scores\"], logreg[\"va_scores\"], log_scale=True)\n",
    "\n",
    "best_lambda = lambdas[np.argmax(logreg[\"va_scores\"])]\n",
    "print('Best lambda:', best_lambda)\n",
    "print('Test score:', \n",
    "      LogisticRegression(C=best_lambda)\n",
    "      .fit(X_train.data.numpy(), (y_train.data == 1)[:, 1].numpy())\n",
    "      .score(X_test.data.numpy(), (y_test.data == 1)[:, 1].numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
